# -*- coding: utf-8 -*-
"""rainfall_prediction_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1be4OIK9g7JbKC-2u6I56Iai_ex53J2kZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, f1_score, confusion_matrix
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
import keras_tuner as kt
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('/content/DEHRADUN, UK, INDIA.csv')
df.head()

df.tail()

df.columns

df.info()

df.describe()

df.isnull().sum()

"""**step** 1: Data Preprocessing"""

def load_and_preprocess_data(file_path):
    # Load dataset
    df = pd.read_csv(file_path, parse_dates=['datetime'])

    # Convert datetime to proper format and set as index
    df['datetime'] = pd.to_datetime(df['datetime'], format='%d-%m-%Y')
    df.set_index('datetime', inplace=True)

    # Select relevant features
    features = ['temp', 'dew', 'humidity', 'precip', 'windspeedmean', 'sealevelpressure', 'cloudcover']
    df = df[features]

    # Handle missing values
    df.fillna(method='ffill', inplace=True)

    # Define rainfall categories based on thresholds
    def categorize_rainfall(precip):
        if precip == 0:
          return "No rainfall"
        elif 0.1 <= precip <= 2.49:
          return "Light rain (drizzle)"
        elif 2.50 <= precip <= 7.59:
          return "Light rain"
        elif 7.60 <= precip <= 35.59:
          return "Moderate rain"
        elif 35.60 <= precip <= 64.49:
          return "Heavy rain"
        elif 64.50 <= precip <= 124.4:
          return "Very heavy rain"
        else:
          return "Extremely heavy rain"

    df['rain_category'] = df['precip'].apply(categorize_rainfall)

    # Feature engineering
    df['dew_point_spread'] = df['temp'] - df['dew']
    df['humidity_cloudcover'] = df['humidity'] * df['cloudcover']
    features = ['temp', 'dew', 'humidity', 'precip', 'windspeedmean', 'sealevelpressure',
                'cloudcover', 'dew_point_spread', 'humidity_cloudcover']
    df = df[features + ['rain_category']]
    return df, categorize_rainfall

# Run the function
file_path = '/content/DEHRADUN, UK, INDIA.csv'
df, categorize_rainfall = load_and_preprocess_data(file_path)
print("Preprocessed DataFrame:")
print(df.head())

"""**step** 2: Exploratory Data Analysis (EDA)"""

def analyze_correlations(df):
    # Compute correlation matrix
    numeric_df = df.select_dtypes(include=np.number)
    correlation_matrix = numeric_df.corr()

    # Plot heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Correlation Matrix of Features')
    plt.savefig('correlation_heatmap.png')
    plt.close()

    # Print correlations with precip
    print("Correlation with precip:")
    print(correlation_matrix['precip'].sort_values(ascending=False))

def perform_eda(df):
    plt.figure(figsize=(12, 6))
    plt.plot(df.index, df['precip'], label='Rainfall (mm)')
    plt.title('Daily Rainfall Over Time')
    plt.xlabel('Date')
    plt.ylabel('Rainfall (mm)')
    plt.legend()
    plt.savefig('rainfall_time_series.png')
    plt.show()

    # Seasonal decomposition
    decomposition = seasonal_decompose(df['precip'], model='additive', period=365)
    decomposition.plot()
    plt.savefig('seasonal_decomposition.png')
    plt.show()

    # Correlation analysis
    analyze_correlations(df)

# Run EDA
perform_eda(df)

"""**step3**: Arima Model for Regression"""

def arima_forecast(df, forecast_days=150):
    # Ensure the index is datetime
    if not pd.api.types.is_datetime64_any_dtype(df.index):
        df.index = pd.to_datetime(df.index)


    # Find best p, d, q
    warnings.filterwarnings("ignore")  # Ignore convergence warnings

    best_aic = float('inf')
    best_order = None

    # Define ranges to search
    p_values = range(0, 5)
    d_values = range(0, 3)
    q_values = range(0, 5)

    for p in p_values:
        for d in d_values:
            for q in q_values:
                try:
                    model = ARIMA(df['precip'], order=(p,d,q))
                    model_fit = model.fit()
                    if model_fit.aic < best_aic:
                        best_aic = model_fit.aic
                        best_order = (p, d, q)
                except:
                    continue

    print(f"Best ARIMA order found: {best_order} with AIC: {best_aic}")

    # Fit the best ARIMA model
    best_model = ARIMA(df['precip'], order=best_order)
    best_model_fit = best_model.fit()

    # Forecast
    forecast = model_fit.forecast(steps=forecast_days)

    # Create forecast index
    last_date = df.index[-1]
    forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_days, freq='D')

    return forecast, forecast_index

# Run ARIMA forecast
arima_pred, forecast_index = arima_forecast(df)
print("ARIMA Forecast (first 15 days):")
print(arima_pred[:15])

"""**step**4:LSTM Model for Regression"""

def build_lstm_model(hp):
    model = Sequential()
    model.add(LSTM(units=hp.Int('units', 50, 200, step=50),
                   return_sequences=True, input_shape=(30, 1)))
    model.add(Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)))
    model.add(LSTM(units=hp.Int('units2', 50, 100, step=25)))
    model.add(Dense(1))
    model.compile(optimizer=tf.keras.optimizers.Adam(
        hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')), loss='mse')
    return model

def lstm_forecast(df, forecast_days=150, model=None):
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df[['precip']].values)

    def create_sequences(data, seq_length):
        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i:i+seq_length])
            y.append(data[i+seq_length])
        return np.array(X), np.array(y)

    seq_length = 30
    X, y = create_sequences(scaled_data, seq_length)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

    if model is None:
        # Build LSTM model
        model = Sequential()
        model.add(LSTM(100, return_sequences=True, input_shape=(seq_length, 1)))
        model.add(Dropout(0.2))  # Add dropout to prevent overfitting
        model.add(LSTM(50))
        model.add(Dropout(0.2))
        model.add(Dense(25, activation='relu'))
        model.add(Dense(1))

        # Custom optimizer with tuned learning rate
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
        model.compile(optimizer=optimizer, loss='mse')

        # Early stopping
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        # Train model
        model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.1,
                  callbacks=[early_stopping], verbose=1)

    # Forecast
    last_sequence = scaled_data[-seq_length:]
    forecast = []

    for _ in range(forecast_days):
        last_sequence_reshaped = last_sequence.reshape((1, seq_length, 1))
        next_pred = model.predict(last_sequence_reshaped, verbose=0)
        forecast.append(next_pred[0, 0])
        last_sequence = np.roll(last_sequence, -1)
        last_sequence[-1] = next_pred

    # Inverse transform
    forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))

    # Create forecast index
    last_date = df.index[-1]
    forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_days, freq='D')

    # Evaluate on test set
    test_predictions = model.predict(X_test, verbose=0)
    test_predictions = scaler.inverse_transform(test_predictions)
    y_test_inv = scaler.inverse_transform(y_test)
    mse = mean_squared_error(y_test_inv, test_predictions)
    print(f"LSTM Test MSE: {mse:.4f}")

    return forecast.flatten(), forecast_index

# LSTM hyperparameter tuning
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df[['precip']].values)

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

seq_length = 30
X, y = create_sequences(scaled_data, seq_length)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

tuner = kt.Hyperband(build_lstm_model, objective='val_loss', max_epochs=50, directory='lstm_tuner')
tuner.search(X_train, y_train, epochs=50, validation_split=0.1, callbacks=[EarlyStopping(patience=5)])
best_model = tuner.get_best_models(num_models=1)[0]

# LSTM forecast with tuned model
lstm_pred, _ = lstm_forecast(df, model=best_model)
print("LSTM Forecast (first 15 days):")
print(lstm_pred[:15])

"""**step 5**:Binary Classification (Rain vs. No Rain)"""

def binary_classification(df):
    # Create binary target (1 for rain, 0 for no rain)
    df['rain_binary'] = (df['precip'] > 0).astype(int)

    # Features and target
    X = df[['temp', 'dew', 'humidity', 'windspeedmean', 'sealevelpressure',
            'cloudcover', 'dew_point_spread', 'humidity_cloudcover']]
    y = df['rain_binary']

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    # Train Random Forest
    clf = RandomForestClassifier(random_state=42)

    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    best_clf = grid_search.best_estimator_
    print("Best Random Forest Parameters:", grid_search.best_params_)

    y_pred = best_clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    feature_importance = pd.Series(best_clf.feature_importances_, index=X.columns).sort_values(ascending=False)
    print("Feature Importance:\n", feature_importance)
    return best_clf, accuracy, f1, X_test, y_test

# Run binary classification
clf, accuracy, f1, X_test, y_test = binary_classification(df)
print(f"Binary Classification - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}")

"""**step6t**:Model Evaluation"""

def evaluate_models(df, clf, X_test, y_test, arima_pred, lstm_pred, forecast_index):
    # Classification evaluation
    y_pred = clf.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:\n", cm)

    # Regression evaluation (compare last 30 days of actual vs. forecast)
    actual = df['precip'][-30:].values
    if len(actual) == len(arima_pred):
        arima_mse = mean_squared_error(actual, arima_pred[:len(actual)])
        lstm_mse = mean_squared_error(actual, lstm_pred[:len(actual)])
        print(f"ARIMA Forecast MSE: {arima_mse:.4f}")
        print(f"LSTM Forecast MSE: {lstm_mse:.4f}")

    # Plot confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix - Random Forest')
    plt.savefig('confusion_matrix.png')
    plt.show()

# Run evaluation
evaluate_models(df, clf, X_test, y_test, arima_pred, lstm_pred, forecast_index)

"""**step7**:Visualization of Forecast"""

def plot_forecast(df, arima_forecast, lstm_forecast, forecast_index):
    plt.figure(figsize=(12, 6))
    plt.plot(df.index[-100:], df['precip'][-100:], label='Actual Rainfall', color='blue')
    plt.plot(forecast_index, arima_forecast, label='ARIMA Forecast', color='red')
    plt.plot(forecast_index, lstm_forecast, label='LSTM Forecast', color='green')
    plt.title('Rainfall Forecast vs Actual')
    plt.xlabel('Date')
    plt.ylabel('Rainfall (mm)')
    plt.legend()
    plt.savefig('forecast_plot.png')
    plt.close()

# Run forecast visualization
plot_forecast(df, arima_pred, lstm_pred, forecast_index)

# Create and save forecast DataFrame
forecast_df = pd.DataFrame({
    'Date': forecast_index,
    'ARIMA_Forecast_mm': arima_pred,
    'LSTM_Forecast_mm': lstm_pred,
    'Rain_Category': [categorize_rainfall(x) for x in lstm_pred]
})
forecast_df.to_csv('rainfall_forecast_output.csv')
print("Forecast output saved to 'rainfall_forecast_output.csv'")

# Print final results
print(f"Binary Classification - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}")
print("Forecast output saved to 'rainfall_forecast_output.csv'")
print("Plots saved: 'rainfall_time_series.png', 'seasonal_decomposition.png', 'forecast_plot.png', 'correlation_heatmap.png', 'confusion_matrix.png'")

# Commented out IPython magic to ensure Python compatibility.
#this forecast plot is for 30 days

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline
def read_and_show_png(image_path):
  """Reads and displays a PNG image.

  Args:
    image_path: The path to the PNG image file.
  """
  try:
    img = mpimg.imread('/content/forecast_plot.png')
    plt.imshow(img)
    plt.axis('off')
    plt.show()
  except FileNotFoundError:
    print(f"Error: Image file not found at {'/content/forecast_plot.png'}")
  except Exception as e:
    print(f"An error occurred: {e}")


read_and_show_png('/content/forecast_plot.png')

forecast=pd.read_csv('/content/rainfall_forecast_output.csv')
forecast

